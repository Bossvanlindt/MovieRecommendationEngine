{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save to ChromaDB with just embedding-id pairs\n",
    "\n",
    "Reduces space usage by ~3x by not storing the documents but pre-calculating embeddings first, as we store the documents in the Azure db anyways. \n",
    "\n",
    "By adding in comp585_movies.csv (from Kaggle) and azure_movies_oct22_filtered.csv (from Azure movies dump on that date), we have a total of 23,909 movies in ChromaDB already (out of ~27K total)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-24T00:51:38.328602Z",
     "start_time": "2023-10-24T00:51:38.324681Z"
    }
   },
   "outputs": [],
   "source": [
    "import chromadb\n",
    "from chromadb.utils import embedding_functions\n",
    "import json\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-24T00:51:38.351077Z",
     "start_time": "2023-10-24T00:51:38.329396Z"
    }
   },
   "outputs": [],
   "source": [
    "client = chromadb.PersistentClient(path=\"../inference/chromadb_test2\")\n",
    "movies = client.get_or_create_collection(\"movies\",metadata={\"hnsw:space\":\"cosine\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-24T00:51:38.354827Z",
     "start_time": "2023-10-24T00:51:38.348564Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "0"
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-24T00:51:38.611785Z",
     "start_time": "2023-10-24T00:51:38.353736Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load the data\n",
    "df = pd.read_csv(\"./kaggle_dataset/comp585_movies_final.csv\")\n",
    "# Drop duplicates\n",
    "df = df.drop_duplicates(subset=['movie_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "df_cleaned = df[['movie_id', 'overview', 'title', 'genres', 'production_companies' , 'production_countries', 'spoken_languages']]\n",
    "#Drop rows with NaN\n",
    "cols_to_check = ['movie_id', 'overview', 'title', 'genres', 'production_companies', 'production_countries', 'spoken_languages']\n",
    "df_cleaned = df_cleaned.dropna(subset=cols_to_check)\n",
    "def convert(text):\n",
    "    try:\n",
    "        if isinstance(text, list):\n",
    "            # If the input is a list, convert it to a JSON string\n",
    "            text = json.dumps(text)\n",
    "        # Use json.loads to parse the string as a JSON object\n",
    "        list_of_dicts = json.loads(text.replace(\"'\", \"\\\"\"))\n",
    "        # Extract the 'name' values from the dictionaries\n",
    "        names = [item['name'] for item in list_of_dicts]\n",
    "        return names\n",
    "    except (json.JSONDecodeError, ValueError):\n",
    "        return []\n",
    "df_cleaned['genres'] = df_cleaned['genres'].apply(convert)\n",
    "df_cleaned['production_countries'] = df_cleaned['production_countries'].apply(convert)\n",
    "df_cleaned['production_companies'] = df_cleaned['production_companies'].apply(convert)\n",
    "df_cleaned['spoken_languages'] = df_cleaned['spoken_languages'].apply(convert)\n",
    "df_cleaned['genres'] = df_cleaned['genres'].apply(lambda x:[i.replace(' ','') for i in x])\n",
    "df_cleaned['production_countries'] = df_cleaned['production_countries'].apply(lambda x:[i.replace(' ','') for i in x])\n",
    "df_cleaned['production_companies'] = df_cleaned['production_companies'].apply(lambda x:[i.replace(' ','') for i in x])\n",
    "df_cleaned['overview'] = df_cleaned['overview'].str.lower()\n",
    "df_cleaned['overview'] = df_cleaned['overview'].apply(lambda x: x.split() if isinstance(x, str) else [])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-24T00:51:39.040807Z",
     "start_time": "2023-10-24T00:51:38.625829Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/tv/hrmwg4bx2f5fwpdnkhv550f80000gn/T/ipykernel_40125/2378863637.py:8: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df_cleaned = df_cleaned.applymap(list_to_string)\n"
     ]
    }
   ],
   "source": [
    "# Convert list of strings to string\n",
    "def list_to_string(lst):\n",
    "    if isinstance(lst, list):\n",
    "        return ' '.join(lst)\n",
    "    return lst\n",
    "\n",
    "# Apply the function to columns in the DataFrame that contain lists\n",
    "df_cleaned = df_cleaned.applymap(list_to_string)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-24T00:51:39.101939Z",
     "start_time": "2023-10-24T00:51:39.065981Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "# Combine all the text columns into one\n",
    "df_cleaned['tags'] = df_cleaned['overview'] +\" \"+ df_cleaned['genres'] +\" \"+ df_cleaned['production_countries'] +\" \"+ df_cleaned['production_companies'] +\" \"+ df_cleaned['spoken_languages']\n",
    "\n",
    "# --- TEXT PROCESSING ---\n",
    "movie_combined = df_cleaned[['movie_id', 'tags']]\n",
    "jsons = df_cleaned['tags'].tolist()\n",
    "# Get the ids\n",
    "ids = df_cleaned['movie_id'].tolist()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-24T00:51:39.133842Z",
     "start_time": "2023-10-24T00:51:39.117172Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-24T00:59:27.217100Z",
     "start_time": "2023-10-24T00:51:39.122324Z"
    }
   },
   "outputs": [],
   "source": [
    "# Get embeddings for all\n",
    "default_ef = embedding_functions.DefaultEmbeddingFunction()\n",
    "embeddings = default_ef(jsons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-24T00:59:27.220561Z",
     "start_time": "2023-10-24T00:59:27.218429Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "17491"
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-24T01:01:07.370146Z",
     "start_time": "2023-10-24T01:01:00.643224Z"
    }
   },
   "outputs": [],
   "source": [
    "# Store all movies to ChromaDB with just embeddings and ids, no documents\n",
    "movies.add(\n",
    "    embeddings=embeddings,\n",
    "    ids=ids\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
